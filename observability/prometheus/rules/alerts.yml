# Prometheus Alert Rules for Noble NovaCoreAI
groups:
  - name: service_health
    interval: 30s
    rules:
      # Service down alerts
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 1 minute."

      # High error rate
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} is experiencing {{ $value }}% error rate."

      # High latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            rate(http_request_duration_seconds_bucket[5m])
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on {{ $labels.job }}"
          description: "95th percentile latency is {{ $value }}s on {{ $labels.job }}."

  - name: database_health
    interval: 30s
    rules:
      # PostgreSQL down
      - alert: PostgresDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been unavailable for more than 1 minute."

      # High connection usage
      - alert: HighDatabaseConnections
        expr: |
          (pg_stat_database_numbackends / pg_settings_max_connections) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High PostgreSQL connection usage"
          description: "PostgreSQL is using {{ $value | humanizePercentage }} of available connections."

      # Redis down
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis cache has been unavailable for more than 1 minute."

      # Redis high memory usage
      - alert: RedisHighMemory
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis high memory usage"
          description: "Redis is using {{ $value | humanizePercentage }} of allocated memory."

  - name: llm_performance
    interval: 30s
    rules:
      # Ollama service unavailable
      - alert: OllamaServiceUnavailable
        expr: |
          sum(rate(http_requests_total{job="intelligence",path="/chat/message",status="503"}[5m])) > 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Ollama service unavailable"
          description: "Intelligence service is returning 503 errors, Ollama may be down."

      # High token usage
      - alert: HighTokenUsage
        expr: |
          rate(llm_tokens_used_total[1h]) > 1000000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High LLM token usage"
          description: "Token usage is {{ $value }} tokens per hour."

  - name: memory_system
    interval: 30s
    rules:
      # Memory storage quota exceeded
      - alert: MemoryStorageQuotaWarning
        expr: |
          (memory_storage_used_bytes / memory_storage_quota_bytes) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Memory storage quota warning for user {{ $labels.user_id }}"
          description: "User {{ $labels.user_id }} is using {{ $value | humanizePercentage }} of allocated memory storage."

      # Embedding service slow
      - alert: EmbeddingServiceSlow
        expr: |
          histogram_quantile(0.95, 
            rate(embedding_generation_duration_seconds_bucket[5m])
          ) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Embedding generation is slow"
          description: "95th percentile embedding generation time is {{ $value }}s."

  - name: resource_usage
    interval: 30s
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}."

      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }}."

      # Disk space low
      - alert: DiskSpaceLow
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 15
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Only {{ $value }}% disk space remaining on {{ $labels.instance }}."

      # Disk space critical
      - alert: DiskSpaceCritical
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Only {{ $value }}% disk space remaining on {{ $labels.instance }}."
